{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676ea7ab",
   "metadata": {},
   "source": [
    "1. Take an input matrix with size 5x5 and a kernel with size 3x3, perform convolution with\n",
    "stride being 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b435ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution (stride=1):\n",
      " [[2. 1. 2.]\n",
      " [1. 2. 3.]\n",
      " [4. 2. 1.]]\n",
      "Convolution (stride=2):\n",
      " [[2. 2.]\n",
      " [4. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define input matrix and kernel\n",
    "input_matrix = np.array([[1, 0, 1, 1, 0],\n",
    "                         [0, 0, 0, 1, 1],\n",
    "                         [1, 0, 0, 0, 1],\n",
    "                         [0, 1, 1, 1, 0],\n",
    "                         [1, 1, 0, 1, 0]])\n",
    "\n",
    "kernel = np.array([[1, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [1, 1, 0]])\n",
    "\n",
    "def conv(X, K, stride=1):\n",
    "    X_h, X_w = X.shape\n",
    "    K_h, K_w = K.shape\n",
    "    out_h = (X_h - K_h) // stride + 1\n",
    "    out_w = (X_w - K_w) // stride + 1\n",
    "    out = np.zeros((out_h, out_w))\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = X[i*stride:i*stride+K_h, j*stride:j*stride+K_w]\n",
    "            out[i, j] = np.sum(region * K)\n",
    "    return out\n",
    "\n",
    "conv_stride_1 = conv(input_matrix, kernel, stride=1)\n",
    "conv_stride_2 = conv(input_matrix, kernel, stride=2)\n",
    "\n",
    "print('Convolution (stride=1):\\n', conv_stride_1)\n",
    "print('Convolution (stride=2):\\n', conv_stride_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde7471",
   "metadata": {},
   "source": [
    "2. Apply max-pooling, average-pooling and sum-pooling to the results from above\n",
    "convolutions [Q1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18478ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Pool (stride=1):\n",
      " [[2. 3.]\n",
      " [4. 1.]]\n",
      "Avg Pool (stride=1):\n",
      " [[1.5  1.25]\n",
      " [1.5  0.25]]\n",
      "Sum Pool (stride=1):\n",
      " [[6. 5.]\n",
      " [6. 1.]]\n",
      "Max Pool (stride=2):\n",
      " [[4.]]\n",
      "Avg Pool (stride=2):\n",
      " [[2.25]]\n",
      "Sum Pool (stride=2):\n",
      " [[9.]]\n"
     ]
    }
   ],
   "source": [
    "def pool(X, pool_size=2, mode='max', stride=1):\n",
    "    h, w = X.shape\n",
    "    out_h = (h - pool_size) // stride + 1\n",
    "    out_w = (w - pool_size) // stride + 1\n",
    "    out = np.zeros((out_h, out_w))\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = X[i*stride:i*stride+pool_size, j*stride:j*stride+pool_size]\n",
    "            if mode == 'max':\n",
    "                out[i, j] = np.max(region)\n",
    "            elif mode == 'avg':\n",
    "                out[i, j] = np.mean(region)\n",
    "            elif mode == 'sum':\n",
    "                out[i, j] = np.sum(region)\n",
    "    return out\n",
    "\n",
    "# added a zero padding to conv_stride_1 to make it 4x4 for non-overlapping pooling\n",
    "conv_stride_1_padded = np.pad(conv_stride_1, ((0,1),(0,1)), mode='constant')\n",
    "\n",
    "max_pool_1 = pool(conv_stride_1_padded, pool_size=2, mode='max', stride=2)\n",
    "avg_pool_1 = pool(conv_stride_1_padded, pool_size=2, mode='avg', stride=2)\n",
    "sum_pool_1 = pool(conv_stride_1_padded, pool_size=2, mode='sum', stride=2)\n",
    "\n",
    "# pooling for stride=2 convolution\n",
    "max_pool_2 = pool(conv_stride_2, pool_size=2, mode='max')\n",
    "avg_pool_2 = pool(conv_stride_2, pool_size=2, mode='avg')\n",
    "sum_pool_2 = pool(conv_stride_2, pool_size=2, mode='sum')\n",
    "\n",
    "print('Max Pool (stride=1):\\n', max_pool_1)\n",
    "print('Avg Pool (stride=1):\\n', avg_pool_1)\n",
    "print('Sum Pool (stride=1):\\n', sum_pool_1)\n",
    "\n",
    "print('Max Pool (stride=2):\\n', max_pool_2)\n",
    "print('Avg Pool (stride=2):\\n', avg_pool_2)\n",
    "print('Sum Pool (stride=2):\\n', sum_pool_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae78bab",
   "metadata": {},
   "source": [
    "3. Visualize the flattened version of the pooled feature maps from [Q2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eae76505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Max Pool (stride=1): [2. 3. 4. 1.]\n",
      "Flattened Avg Pool (stride=1): [1.5  1.25 1.5  0.25]\n",
      "Flattened Sum Pool (stride=1): [6. 5. 6. 1.]\n",
      "Flattened Max Pool (stride=2): [4.]\n",
      "Flattened Avg Pool (stride=2): [2.25]\n",
      "Flattened Sum Pool (stride=2): [9.]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten pooled feature maps\n",
    "flat_max_pool_1 = max_pool_1.flatten()\n",
    "flat_avg_pool_1 = avg_pool_1.flatten()\n",
    "flat_sum_pool_1 = sum_pool_1.flatten()\n",
    "\n",
    "flat_max_pool_2 = max_pool_2.flatten()\n",
    "flat_avg_pool_2 = avg_pool_2.flatten()\n",
    "flat_sum_pool_2 = sum_pool_2.flatten()\n",
    "\n",
    "print('Flattened Max Pool (stride=1):', flat_max_pool_1)\n",
    "print('Flattened Avg Pool (stride=1):', flat_avg_pool_1)\n",
    "print('Flattened Sum Pool (stride=1):', flat_sum_pool_1)\n",
    "\n",
    "print('Flattened Max Pool (stride=2):', flat_max_pool_2)\n",
    "print('Flattened Avg Pool (stride=2):', flat_avg_pool_2)\n",
    "print('Flattened Sum Pool (stride=2):', flat_sum_pool_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285db67",
   "metadata": {},
   "source": [
    "4. With weights being randomly chosen and bias being 1, learn the weights and bias over an\n",
    "epoch for the flattened arrays from [Q3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cc73109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=22.01879993172998, Weights=[ 0.44978996 -0.20865059  0.55384015  1.49956776], Bias=[0.9765379 0.9765379 0.9765379 0.9765379]\n",
      "Epoch 2: Loss=15.721973621253504, Weights=[ 0.41013901 -0.26812701  0.47453826  1.47974229], Bias=[0.95671243 0.95671243 0.95671243 0.95671243]\n",
      "Epoch 3: Loss=11.225882214915531, Weights=[ 0.37663397 -0.31838458  0.40752817  1.46298976], Bias=[0.93995991 0.93995991 0.93995991 0.93995991]\n",
      "Epoch 4: Loss=8.015560548505062, Weights=[ 0.3483222  -0.36085223  0.35090464  1.44883388], Bias=[0.92580402 0.92580402 0.92580402 0.92580402]\n",
      "Epoch 5: Loss=5.723310620646326, Weights=[ 0.32439876 -0.39673739  0.30305775  1.43687216], Bias=[0.9138423 0.9138423 0.9138423 0.9138423]\n",
      "Epoch 6: Loss=4.0865868659069955, Weights=[ 0.30418345 -0.42706035  0.26262714  1.42676451], Bias=[0.90373465 0.90373465 0.90373465 0.90373465]\n",
      "Epoch 7: Loss=2.917925186929242, Weights=[ 0.28710152 -0.45268326  0.22846327  1.41822354], Bias=[0.89519368 0.89519368 0.89519368 0.89519368]\n",
      "Epoch 8: Loss=2.083471531597151, Weights=[ 0.27266728 -0.47433461  0.19959479  1.41100642], Bias=[0.88797656 0.88797656 0.88797656 0.88797656]\n",
      "Epoch 9: Loss=1.4876507603486566, Weights=[ 0.26047035 -0.49263     0.17520094  1.40490796], Bias=[0.8818781 0.8818781 0.8818781 0.8818781]\n",
      "Epoch 10: Loss=1.0622198341579494, Weights=[ 0.25016395 -0.50808961  0.15458813  1.39975475], Bias=[0.8767249 0.8767249 0.8767249 0.8767249]\n",
      "Epoch 11: Loss=0.7584515170846304, Weights=[ 0.24145503 -0.52115298  0.1371703   1.3954003 ], Bias=[0.87237044 0.87237044 0.87237044 0.87237044]\n",
      "Epoch 12: Loss=0.5415533444863534, Weights=[ 0.234096   -0.53219152  0.12245224  1.39172078], Bias=[0.86869093 0.86869093 0.86869093 0.86869093]\n",
      "Epoch 13: Loss=0.38668262679686816, Weights=[ 0.22787762 -0.5415191   0.11001548  1.38861159], Bias=[0.86558174 0.86558174 0.86558174 0.86558174]\n",
      "Epoch 14: Loss=0.27610106259863354, Weights=[ 0.22262309 -0.54940089  0.09950641  1.38598433], Bias=[0.86295447 0.86295447 0.86295447 0.86295447]\n",
      "Epoch 15: Loss=0.1971430612219896, Weights=[ 0.21818301 -0.55606101  0.09062626  1.38376429], Bias=[0.86073443 0.86073443 0.86073443 0.86073443]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x = flat_max_pool_1\n",
    "w = np.random.randn(len(x))\n",
    "b = 1.0\n",
    "target = np.ones_like(x)  # dummy target \n",
    "lr = 0.01\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = np.dot(x, w) + b\n",
    "    loss = np.mean((y_pred - target)**2)\n",
    "    \n",
    "    # Backward pass - gradient descent\n",
    "    grad_w = 2 * (y_pred - target) * x / len(x)\n",
    "    grad_b = 2 * (y_pred - target) / len(x)\n",
    "    \n",
    "    w -= lr * grad_w\n",
    "    b -= lr * grad_b\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Loss={loss}, Weights={w}, Bias={b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17628cd5",
   "metadata": {},
   "source": [
    "5. Design and implement a Convolutional Neural Network (CNN) to classify images from\n",
    "MNIST, CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb48c741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Suppress all warnings to prevent kernel crash from protobuf warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Silence TensorFlow startup messages\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    print(\"TensorFlow imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error importing TensorFlow: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f7acc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9514 - loss: 0.1679 - val_accuracy: 0.9845 - val_loss: 0.0584\n",
      "Epoch 2/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9514 - loss: 0.1679 - val_accuracy: 0.9845 - val_loss: 0.0584\n",
      "Epoch 2/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0604 - val_accuracy: 0.9860 - val_loss: 0.0540\n",
      "Epoch 3/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0604 - val_accuracy: 0.9860 - val_loss: 0.0540\n",
      "Epoch 3/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0403 - val_accuracy: 0.9765 - val_loss: 0.0873\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0403 - val_accuracy: 0.9765 - val_loss: 0.0873\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - accuracy: 0.9732 - loss: 0.0810\n",
      "MNIST Test accuracy: 0.9732000231742859\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - accuracy: 0.9732 - loss: 0.0810\n",
      "MNIST Test accuracy: 0.9732000231742859\n"
     ]
    }
   ],
   "source": [
    "# MNIST Model\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[..., np.newaxis] / 255.0\n",
    "x_test = x_test[..., np.newaxis] / 255.0\n",
    "\n",
    "model_mnist = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_mnist.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model_mnist.fit(x_train, y_train, epochs=3, validation_split=0.1)\n",
    "test_loss, test_acc = model_mnist.evaluate(x_test, y_test)\n",
    "print('MNIST Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "988778ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 18ms/step - accuracy: 0.5056 - loss: 1.3767 - val_accuracy: 0.6484 - val_loss: 1.0117\n",
      "Epoch 2/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 18ms/step - accuracy: 0.5056 - loss: 1.3767 - val_accuracy: 0.6484 - val_loss: 1.0117\n",
      "Epoch 2/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 19ms/step - accuracy: 0.6658 - loss: 0.9510 - val_accuracy: 0.7060 - val_loss: 0.8603\n",
      "Epoch 3/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 19ms/step - accuracy: 0.6658 - loss: 0.9510 - val_accuracy: 0.7060 - val_loss: 0.8603\n",
      "Epoch 3/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 19ms/step - accuracy: 0.7243 - loss: 0.7862 - val_accuracy: 0.7130 - val_loss: 0.8338\n",
      "Epoch 4/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 19ms/step - accuracy: 0.7243 - loss: 0.7862 - val_accuracy: 0.7130 - val_loss: 0.8338\n",
      "Epoch 4/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 20ms/step - accuracy: 0.7621 - loss: 0.6795 - val_accuracy: 0.7430 - val_loss: 0.7512\n",
      "Epoch 5/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 20ms/step - accuracy: 0.7621 - loss: 0.6795 - val_accuracy: 0.7430 - val_loss: 0.7512\n",
      "Epoch 5/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 21ms/step - accuracy: 0.7963 - loss: 0.5810 - val_accuracy: 0.7334 - val_loss: 0.7900\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 21ms/step - accuracy: 0.7963 - loss: 0.5810 - val_accuracy: 0.7334 - val_loss: 0.7900\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7160 - loss: 0.8413\n",
      "CIFAR-10 Test accuracy: 0.7160000205039978\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7160 - loss: 0.8413\n",
      "CIFAR-10 Test accuracy: 0.7160000205039978\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 Model\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train_c, y_train_c), (x_test_c, y_test_c) = cifar10.load_data()\n",
    "x_train_c = x_train_c / 255.0\n",
    "x_test_c = x_test_c / 255.0\n",
    "\n",
    "model_cifar = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_cifar.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model_cifar.fit(x_train_c, y_train_c, epochs=5, validation_split=0.1)\n",
    "test_loss_c, test_acc_c = model_cifar.evaluate(x_test_c, y_test_c)\n",
    "print('CIFAR-10 Test accuracy:', test_acc_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbd344",
   "metadata": {},
   "source": [
    "# testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38a8770c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model testing functions are ready. Use test_mnist_image() and test_cifar10_image() with image paths.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# MNIST Image Classification Test\n",
    "def test_mnist_image(model, img_path=None):\n",
    "    if img_path is None:\n",
    "        print(\"Please provide an image path to test MNIST model\")\n",
    "        return\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image from {img_path}\")\n",
    "        return\n",
    "    img_resized = cv2.resize(img, (28, 28))\n",
    "    plt.imshow(img_resized, cmap='gray')\n",
    "    plt.title('Input Image for MNIST')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    img_array = img_resized.astype('float32') / 255.0\n",
    "    img_array = img_array.reshape(1, 28, 28, 1)\n",
    "    pred = model.predict(img_array)\n",
    "    print('Predicted digit:', pred.argmax())\n",
    "\n",
    "# CIFAR-10 Image Classification Test\n",
    "def test_cifar10_image(model, img_path=None):\n",
    "    if img_path is None:\n",
    "        print(\"Please provide an image path to test CIFAR-10 model\")\n",
    "        return\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image from {img_path}\")\n",
    "        return\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (32, 32))\n",
    "    plt.imshow(img_resized)\n",
    "    plt.title('Input Image for CIFAR-10')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    img_array = img_resized.astype('float32') / 255.0\n",
    "    img_array = img_array.reshape(1, 32, 32, 3)\n",
    "    pred = model.predict(img_array)\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    print('Predicted class:', class_names[pred.argmax()])\n",
    "\n",
    "# Example usage (uncomment and provide image paths to test)\n",
    "# test_mnist_image(model_mnist, '/path/to/mnist/image.png')\n",
    "# test_cifar10_image(model_cifar, '/path/to/cifar10/image.png')\n",
    "print(\"Model testing functions are ready. Use test_mnist_image() and test_cifar10_image() with image paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da825713",
   "metadata": {},
   "source": [
    "4. With weights being randomly chosen and bias being 1, learn the weights and bias over an\n",
    "epoch for the flattened arrays from [Q3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ded2bb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=22.01879993172998, Weights=[ 0.44978996 -0.20865059  0.55384015  1.49956776], Bias=[0.9765379 0.9765379 0.9765379 0.9765379]\n",
      "Epoch 2: Loss=15.721973621253504, Weights=[ 0.41013901 -0.26812701  0.47453826  1.47974229], Bias=[0.95671243 0.95671243 0.95671243 0.95671243]\n",
      "Epoch 3: Loss=11.225882214915531, Weights=[ 0.37663397 -0.31838458  0.40752817  1.46298976], Bias=[0.93995991 0.93995991 0.93995991 0.93995991]\n",
      "Epoch 4: Loss=8.015560548505062, Weights=[ 0.3483222  -0.36085223  0.35090464  1.44883388], Bias=[0.92580402 0.92580402 0.92580402 0.92580402]\n",
      "Epoch 5: Loss=5.723310620646326, Weights=[ 0.32439876 -0.39673739  0.30305775  1.43687216], Bias=[0.9138423 0.9138423 0.9138423 0.9138423]\n",
      "Epoch 6: Loss=4.0865868659069955, Weights=[ 0.30418345 -0.42706035  0.26262714  1.42676451], Bias=[0.90373465 0.90373465 0.90373465 0.90373465]\n",
      "Epoch 7: Loss=2.917925186929242, Weights=[ 0.28710152 -0.45268326  0.22846327  1.41822354], Bias=[0.89519368 0.89519368 0.89519368 0.89519368]\n",
      "Epoch 8: Loss=2.083471531597151, Weights=[ 0.27266728 -0.47433461  0.19959479  1.41100642], Bias=[0.88797656 0.88797656 0.88797656 0.88797656]\n",
      "Epoch 9: Loss=1.4876507603486566, Weights=[ 0.26047035 -0.49263     0.17520094  1.40490796], Bias=[0.8818781 0.8818781 0.8818781 0.8818781]\n",
      "Epoch 10: Loss=1.0622198341579494, Weights=[ 0.25016395 -0.50808961  0.15458813  1.39975475], Bias=[0.8767249 0.8767249 0.8767249 0.8767249]\n",
      "Epoch 11: Loss=0.7584515170846304, Weights=[ 0.24145503 -0.52115298  0.1371703   1.3954003 ], Bias=[0.87237044 0.87237044 0.87237044 0.87237044]\n",
      "Epoch 12: Loss=0.5415533444863534, Weights=[ 0.234096   -0.53219152  0.12245224  1.39172078], Bias=[0.86869093 0.86869093 0.86869093 0.86869093]\n",
      "Epoch 13: Loss=0.38668262679686816, Weights=[ 0.22787762 -0.5415191   0.11001548  1.38861159], Bias=[0.86558174 0.86558174 0.86558174 0.86558174]\n",
      "Epoch 14: Loss=0.27610106259863354, Weights=[ 0.22262309 -0.54940089  0.09950641  1.38598433], Bias=[0.86295447 0.86295447 0.86295447 0.86295447]\n",
      "Epoch 15: Loss=0.1971430612219896, Weights=[ 0.21818301 -0.55606101  0.09062626  1.38376429], Bias=[0.86073443 0.86073443 0.86073443 0.86073443]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x = flat_max_pool_1\n",
    "w = np.random.randn(len(x))\n",
    "b = 1.0\n",
    "target = np.ones_like(x)  # dummy target \n",
    "lr = 0.01\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = np.dot(x, w) + b\n",
    "    loss = np.mean((y_pred - target)**2)\n",
    "    \n",
    "    # Backward pass - gradient descent\n",
    "    grad_w = 2 * (y_pred - target) * x / len(x)\n",
    "    grad_b = 2 * (y_pred - target) / len(x)\n",
    "    \n",
    "    w -= lr * grad_w\n",
    "    b -= lr * grad_b\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Loss={loss}, Weights={w}, Bias={b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a721fe40",
   "metadata": {},
   "source": [
    "5. Design and implement a Convolutional Neural Network (CNN) to classify images from\n",
    "MNIST, CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11e03465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Suppress all warnings to prevent kernel crash from protobuf warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Silence TensorFlow startup messages\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    print(\"TensorFlow imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error importing TensorFlow: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5f3aee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9465 - loss: 0.1834 - val_accuracy: 0.9822 - val_loss: 0.0655\n",
      "Epoch 2/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9465 - loss: 0.1834 - val_accuracy: 0.9822 - val_loss: 0.0655\n",
      "Epoch 2/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9815 - loss: 0.0629 - val_accuracy: 0.9845 - val_loss: 0.0593\n",
      "Epoch 3/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9815 - loss: 0.0629 - val_accuracy: 0.9845 - val_loss: 0.0593\n",
      "Epoch 3/3\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9872 - loss: 0.0410 - val_accuracy: 0.9832 - val_loss: 0.0632\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9872 - loss: 0.0410 - val_accuracy: 0.9832 - val_loss: 0.0632\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - accuracy: 0.9819 - loss: 0.0579\n",
      "MNIST Test accuracy: 0.9818999767303467\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - accuracy: 0.9819 - loss: 0.0579\n",
      "MNIST Test accuracy: 0.9818999767303467\n"
     ]
    }
   ],
   "source": [
    "# MNIST Model\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[..., np.newaxis] / 255.0\n",
    "x_test = x_test[..., np.newaxis] / 255.0\n",
    "\n",
    "model_mnist = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_mnist.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model_mnist.fit(x_train, y_train, epochs=3, validation_split=0.1)\n",
    "test_loss, test_acc = model_mnist.evaluate(x_test, y_test)\n",
    "print('MNIST Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "847072ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 25ms/step - accuracy: 0.5027 - loss: 1.3832 - val_accuracy: 0.6400 - val_loss: 1.0229\n",
      "Epoch 2/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 25ms/step - accuracy: 0.5027 - loss: 1.3832 - val_accuracy: 0.6400 - val_loss: 1.0229\n",
      "Epoch 2/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 24ms/step - accuracy: 0.6689 - loss: 0.9473 - val_accuracy: 0.7002 - val_loss: 0.8772\n",
      "Epoch 3/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 24ms/step - accuracy: 0.6689 - loss: 0.9473 - val_accuracy: 0.7002 - val_loss: 0.8772\n",
      "Epoch 3/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 24ms/step - accuracy: 0.7308 - loss: 0.7779 - val_accuracy: 0.7330 - val_loss: 0.7841\n",
      "Epoch 4/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 24ms/step - accuracy: 0.7308 - loss: 0.7779 - val_accuracy: 0.7330 - val_loss: 0.7841\n",
      "Epoch 4/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 25ms/step - accuracy: 0.7687 - loss: 0.6684 - val_accuracy: 0.7438 - val_loss: 0.7530\n",
      "Epoch 5/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 25ms/step - accuracy: 0.7687 - loss: 0.6684 - val_accuracy: 0.7438 - val_loss: 0.7530\n",
      "Epoch 5/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 25ms/step - accuracy: 0.7988 - loss: 0.5782 - val_accuracy: 0.7488 - val_loss: 0.7740\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 25ms/step - accuracy: 0.7988 - loss: 0.5782 - val_accuracy: 0.7488 - val_loss: 0.7740\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7312 - loss: 0.8011\n",
      "CIFAR-10 Test accuracy: 0.7311999797821045\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7312 - loss: 0.8011\n",
      "CIFAR-10 Test accuracy: 0.7311999797821045\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 Model\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train_c, y_train_c), (x_test_c, y_test_c) = cifar10.load_data()\n",
    "x_train_c = x_train_c / 255.0\n",
    "x_test_c = x_test_c / 255.0\n",
    "\n",
    "model_cifar = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_cifar.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model_cifar.fit(x_train_c, y_train_c, epochs=5, validation_split=0.1)\n",
    "test_loss_c, test_acc_c = model_cifar.evaluate(x_test_c, y_test_c)\n",
    "print('CIFAR-10 Test accuracy:', test_acc_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f914b5d",
   "metadata": {},
   "source": [
    "# testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8011d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Predicted digit: 3\n",
      "Predicted digit: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Predicted class: dog\n",
      "Model testing functions are ready. Use test_mnist_image() and test_cifar10_image() with image paths.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Predicted class: dog\n",
      "Model testing functions are ready. Use test_mnist_image() and test_cifar10_image() with image paths.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# MNIST Image Classification Test\n",
    "def test_mnist_image(model, img_path=None):\n",
    "    if img_path is None:\n",
    "        print(\"Please provide an image path to test MNIST model\")\n",
    "        return\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image from {img_path}\")\n",
    "        return\n",
    "    img_resized = cv2.resize(img, (28, 28))\n",
    "    plt.imshow(img_resized, cmap='gray')\n",
    "    plt.title('Input Image for MNIST')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    img_array = img_resized.astype('float32') / 255.0\n",
    "    img_array = img_array.reshape(1, 28, 28, 1)\n",
    "    pred = model.predict(img_array)\n",
    "    print('Predicted digit:', pred.argmax())\n",
    "\n",
    "# CIFAR-10 Image Classification Test\n",
    "def test_cifar10_image(model, img_path=None):\n",
    "    if img_path is None:\n",
    "        print(\"Please provide an image path to test CIFAR-10 model\")\n",
    "        return\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image from {img_path}\")\n",
    "        return\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (32, 32))\n",
    "    plt.imshow(img_resized)\n",
    "    plt.title('Input Image for CIFAR-10')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    img_array = img_resized.astype('float32') / 255.0\n",
    "    img_array = img_array.reshape(1, 32, 32, 3)\n",
    "    pred = model.predict(img_array)\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    print('Predicted class:', class_names[pred.argmax()])\n",
    "\n",
    "# Example usage (uncomment and provide image paths to test)\n",
    "test_mnist_image(model_mnist, '/Users/sumith/Desktop/5 sem/NNDL/number3.webp')\n",
    "test_cifar10_image(model_cifar, '/Users/sumith/Desktop/5 sem/NNDL/dog.png')\n",
    "print(\"Model testing functions are ready. Use test_mnist_image() and test_cifar10_image() with image paths.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
