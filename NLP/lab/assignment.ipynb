{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f3083f",
   "metadata": {},
   "source": [
    "## Section 1: Import Libraries and Load Dataset\n",
    "\n",
    "We will use the 20 Newsgroups dataset from scikit-learn for text classification. We select 2 categories (alt.atheism and soc.religion.christian) for binary classification. This dataset contains real-world text documents and is ideal for comparing classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be94c4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 1079 samples\n",
      "Number of classes: 2\n",
      "Class distribution: Class 0: 480, Class 1: 599\n",
      "\n",
      "Sample text (first 200 chars): From: nigel.allen@canrem.com (Nigel Allen)\n",
      "Subject: library of congress to host dead sea scroll symposium april 21-22\n",
      "Lines: 96\n",
      "\n",
      "\n",
      " Library of Congress to Host Dead Sea Scroll Symposium April 21-22\n",
      " To...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load 20 Newsgroups dataset - binary classification (2 categories)\n",
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "X_text = train.data\n",
    "y = train.target\n",
    "\n",
    "print(f\"Dataset shape: {len(X_text)} samples\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Class distribution: Class 0: {np.sum(y==0)}, Class 1: {np.sum(y==1)}\")\n",
    "print(f\"\\nSample text (first 200 chars): {X_text[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2731bb",
   "metadata": {},
   "source": [
    "## Section 2: Preprocess Text Data\n",
    "\n",
    "We use TfidfVectorizer to convert text data into numerical features. This creates a matrix where each row represents a document and each column represents a unique word, with values representing the TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e68094c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (1079, 5000)\n",
      "Number of features extracted: 5000\n"
     ]
    }
   ],
   "source": [
    "# Vectorize text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', lowercase=True)\n",
    "X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features extracted: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c0fe2",
   "metadata": {},
   "source": [
    "## Section 3: K-Fold Cross-Validation Setup\n",
    "\n",
    "We implement 5-fold cross-validation to evaluate both models. This splits the dataset into 5 folds where each fold is used as a test set once while the remaining 4 folds are used for training. This provides more robust evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19fd652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize k-fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for metrics\n",
    "lr_f1_scores = []\n",
    "nn_f1_scores = []\n",
    "lr_accuracy = []\n",
    "nn_accuracy = []\n",
    "lr_precision = []\n",
    "nn_precision = []\n",
    "lr_recall = []\n",
    "nn_recall = []\n",
    "\n",
    "fold_num = 0\n",
    "\n",
    "# Train and evaluate models on each fold\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    fold_num += 1\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Train Logistic Regression\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    lr_pred = lr_model.predict(X_test)\n",
    "    \n",
    "    lr_f1_scores.append(f1_score(y_test, lr_pred))\n",
    "    lr_accuracy.append(accuracy_score(y_test, lr_pred))\n",
    "    lr_precision.append(precision_score(y_test, lr_pred))\n",
    "    lr_recall.append(recall_score(y_test, lr_pred))\n",
    "    \n",
    "    # Train Neural Network\n",
    "    nn_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "    nn_model.fit(X_train, y_train)\n",
    "    nn_pred = nn_model.predict(X_test)\n",
    "    \n",
    "    nn_f1_scores.append(f1_score(y_test, nn_pred))\n",
    "    nn_accuracy.append(accuracy_score(y_test, nn_pred))\n",
    "    nn_precision.append(precision_score(y_test, nn_pred))\n",
    "    nn_recall.append(recall_score(y_test, nn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5facfecb",
   "metadata": {},
   "source": [
    "## Section 4: Cross-Validation Results Summary\n",
    "\n",
    "Display the evaluation metrics (F1-score, Accuracy, Precision, Recall) for each fold and each model. This shows the variability in model performance across different data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a43d2aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results by Fold:\n",
      " Fold  LR F1  NN F1  LR Accuracy  NN Accuracy\n",
      "    1 0.9569 0.9878       0.9491       0.9861\n",
      "    2 0.9794 0.9874       0.9769       0.9861\n",
      "    3 0.9784 0.9868       0.9769       0.9861\n",
      "    4 0.9746 0.9829       0.9722       0.9815\n",
      "    5 0.9699 0.9847       0.9628       0.9814\n"
     ]
    }
   ],
   "source": [
    "# Create results dataframe for each fold\n",
    "results_by_fold = pd.DataFrame({\n",
    "    'Fold': range(1, k_folds + 1),\n",
    "    'LR F1': np.round(lr_f1_scores, 4),\n",
    "    'NN F1': np.round(nn_f1_scores, 4),\n",
    "    'LR Accuracy': np.round(lr_accuracy, 4),\n",
    "    'NN Accuracy': np.round(nn_accuracy, 4)\n",
    "})\n",
    "\n",
    "print(\"Cross-Validation Results by Fold:\")\n",
    "print(results_by_fold.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524550e",
   "metadata": {},
   "source": [
    "## Section 5: Calculate Performance Statistics\n",
    "\n",
    "Compute the mean and standard deviation of F1-scores across all folds for each model. These statistics are essential for conducting the hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84a46750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Statistics Summary:\n",
      "              Model  Mean F1-Score  Std Dev F1-Score  Mean Accuracy  Mean Precision  Mean Recall\n",
      "Logistic Regression         0.9718            0.0092         0.9676          0.9469       0.9983\n",
      "     Neural Network         0.9859            0.0021         0.9842          0.9788       0.9933\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics for F1-scores\n",
    "lr_f1_mean = np.mean(lr_f1_scores)\n",
    "lr_f1_std = np.std(lr_f1_scores, ddof=1)\n",
    "nn_f1_mean = np.mean(nn_f1_scores)\n",
    "nn_f1_std = np.std(nn_f1_scores, ddof=1)\n",
    "\n",
    "# Summary statistics table\n",
    "statistics = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Neural Network'],\n",
    "    'Mean F1-Score': [np.round(lr_f1_mean, 4), np.round(nn_f1_mean, 4)],\n",
    "    'Std Dev F1-Score': [np.round(lr_f1_std, 4), np.round(nn_f1_std, 4)],\n",
    "    'Mean Accuracy': [np.round(np.mean(lr_accuracy), 4), np.round(np.mean(nn_accuracy), 4)],\n",
    "    'Mean Precision': [np.round(np.mean(lr_precision), 4), np.round(np.mean(nn_precision), 4)],\n",
    "    'Mean Recall': [np.round(np.mean(lr_recall), 4), np.round(np.mean(nn_recall), 4)]\n",
    "})\n",
    "\n",
    "print(\"Performance Statistics Summary:\")\n",
    "print(statistics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db64af",
   "metadata": {},
   "source": [
    "## Section 6: Hypothesis Testing - Independent Samples T-Test\n",
    "\n",
    "**Null Hypothesis (H₀):** There is no significant difference between the mean F1-scores of Logistic Regression and Neural Network models.\n",
    "\n",
    "**Alternative Hypothesis (H₁):** There is a significant difference between the mean F1-scores of the two models.\n",
    "\n",
    "**Significance Level:** α = 0.05\n",
    "\n",
    "**Test Type:** Two-tailed independent samples t-test\n",
    "\n",
    "This test assumes:\n",
    "- Both samples are approximately normally distributed\n",
    "- Both samples have roughly equal variances\n",
    "- Observations are independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91fb82c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Test Results:\n",
      "t-statistic: -3.3584\n",
      "p-value: 0.01\n",
      "Significance level (α): 0.05\n",
      "Statistically Significant: True\n",
      "\n",
      "Decision: Reject H₀\n"
     ]
    }
   ],
   "source": [
    "# Perform independent samples t-test\n",
    "t_statistic, p_value = stats.ttest_ind(lr_f1_scores, nn_f1_scores)\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Interpretation\n",
    "is_significant = p_value < alpha\n",
    "\n",
    "print(\"T-Test Results:\")\n",
    "print(f\"t-statistic: {np.round(t_statistic, 4)}\")\n",
    "print(f\"p-value: {np.round(p_value, 4)}\")\n",
    "print(f\"Significance level (α): {alpha}\")\n",
    "print(f\"Statistically Significant: {is_significant}\")\n",
    "print(f\"\\nDecision: {'Reject H₀' if is_significant else 'Fail to reject H₀'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91a72a",
   "metadata": {},
   "source": [
    "## Section 7: Interpretation of T-Test Results\n",
    "\n",
    "**Understanding T-Statistic:**\n",
    "- The t-statistic measures how many standard errors the sample mean difference is from zero\n",
    "- A larger absolute t-value indicates a greater difference between the two model means\n",
    "- Formula: t = (mean₁ - mean₂) / SE, where SE is the standard error\n",
    "\n",
    "**Understanding P-Value:**\n",
    "- The p-value is the probability of observing the data (or more extreme) if the null hypothesis is true\n",
    "- A small p-value (< 0.05) suggests the difference is unlikely due to random chance\n",
    "- If p-value > 0.05, we fail to reject the null hypothesis (no significant difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7846a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERPRETATION OF RESULTS:\n",
      "\n",
      "1. Model Performance Difference:\n",
      "   - LR Mean F1: 0.9718\n",
      "   - NN Mean F1: 0.9859\n",
      "   - Difference: 0.0141\n",
      "\n",
      "2. T-Test Analysis:\n",
      "   - t-statistic = -3.3584: The mean F1-score difference is 3.36 standard errors from zero\n",
      "   - p-value = 0.01: The probability of this difference occurring by chance is 1.0%\n",
      "\n",
      "3. Statistical Conclusion:\n",
      "   Since p-value < α (0.05), we REJECT the null hypothesis.\n",
      "\n",
      "   This means: There IS A statistically significant difference between the models.\n",
      "\n",
      "4. Practical Interpretation:\n",
      "   The difference likely represents a real difference in model performance.\n",
      "   The better-performing model on this text classification task.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed interpretation\n",
    "interpretation_text = f\"\"\"\n",
    "INTERPRETATION OF RESULTS:\n",
    "\n",
    "1. Model Performance Difference:\n",
    "   - LR Mean F1: {np.round(lr_f1_mean, 4)}\n",
    "   - NN Mean F1: {np.round(nn_f1_mean, 4)}\n",
    "   - Difference: {np.round(abs(lr_f1_mean - nn_f1_mean), 4)}\n",
    "\n",
    "2. T-Test Analysis:\n",
    "   - t-statistic = {np.round(t_statistic, 4)}: The mean F1-score difference is {abs(np.round(t_statistic, 2))} standard errors from zero\n",
    "   - p-value = {np.round(p_value, 4)}: The probability of this difference occurring by chance is {np.round(p_value*100, 2)}%\n",
    "\n",
    "3. Statistical Conclusion:\n",
    "   Since p-value {('>' if p_value >= alpha else '<')} α (0.05), we {'FAIL TO REJECT' if p_value >= alpha else 'REJECT'} the null hypothesis.\n",
    "   \n",
    "   This means: There {'IS NO' if p_value >= alpha else 'IS A'} statistically significant difference between the models.\n",
    "\n",
    "4. Practical Interpretation:\n",
    "   The difference {'appears to be due to random variation' if p_value >= alpha else 'likely represents a real difference'} in model performance.\n",
    "   The {'models perform similarly' if p_value >= alpha else 'better-performing model'} on this text classification task.\n",
    "\"\"\"\n",
    "\n",
    "print(interpretation_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447742c",
   "metadata": {},
   "source": [
    "## Section 8: Types of T-Tests\n",
    "\n",
    "**Independent Samples T-Test (Used Here):**\n",
    "- Compares means of two independent groups\n",
    "- Assumes independent observations and equal variances\n",
    "- Formula: t = (μ₁ - μ₂) / √(s₁²/n₁ + s₂²/n₂)\n",
    "\n",
    "**Paired Samples T-Test:**\n",
    "- Compares means of two related/dependent groups (e.g., before-after measurements)\n",
    "- Assumes paired observations\n",
    "\n",
    "**One-Sample T-Test:**\n",
    "- Compares a sample mean against a known population mean\n",
    "\n",
    "**Welch's T-Test:**\n",
    "- Variant of independent samples t-test that doesn't assume equal variances\n",
    "- More robust when sample sizes or variances differ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e53752",
   "metadata": {},
   "source": [
    "## Section 9: Comprehensive Results Table\n",
    "\n",
    "Final summary table combining dataset information, model descriptions, and statistical test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "843ec518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET INFORMATION\n",
      "================================================================================\n",
      " Dataset Name 20 Newsgroups (2 categories)\n",
      "         Task   Binary Text Classification\n",
      "Total Samples                         1079\n",
      "     Features                         5000\n",
      "      Classes                            2\n",
      "Class Balance                     Balanced\n",
      "\n",
      "================================================================================\n",
      "MODEL DESCRIPTIONS\n",
      "================================================================================\n",
      "              Model              Type                      Hyperparameters Complexity\n",
      "Logistic Regression Linear Classifier                        max_iter=1000        Low\n",
      "     Neural Network     Deep Learning hidden_layers=(100,50), max_iter=500       High\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION METRICS (F1-SCORE)\n",
      "================================================================================\n",
      " Fold  LR F1-Score  NN F1-Score  Difference\n",
      "    1       0.9569       0.9878      0.0309\n",
      "    2       0.9794       0.9874      0.0080\n",
      "    3       0.9784       0.9868      0.0084\n",
      "    4       0.9746       0.9829      0.0083\n",
      "    5       0.9699       0.9847      0.0148\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "          Metric  Logistic Regression  Neural Network\n",
      "   Mean F1-Score               0.9718          0.9859\n",
      "Std Dev F1-Score               0.0092          0.0021\n",
      "   Mean Accuracy               0.9676          0.9842\n",
      "  Mean Precision               0.9469          0.9788\n",
      "     Mean Recall               0.9983          0.9933\n",
      "\n",
      "================================================================================\n",
      "INDEPENDENT SAMPLES T-TEST RESULTS\n",
      "================================================================================\n",
      "       t-statistic     -3.3584\n",
      "           p-value        0.01\n",
      "Significance Level        0.05\n",
      "            Result Significant\n",
      "        Conclusion   Reject H₀\n"
     ]
    }
   ],
   "source": [
    "# Dataset Information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "dataset_info = pd.DataFrame({\n",
    "    'Attribute': ['Dataset Name', 'Task', 'Total Samples', 'Features', 'Classes', 'Class Balance'],\n",
    "    'Value': ['20 Newsgroups (2 categories)', 'Binary Text Classification', len(X_text), X.shape[1], 2, 'Balanced']\n",
    "})\n",
    "print(dataset_info.to_string(index=False, header=False))\n",
    "\n",
    "# Model Information\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL DESCRIPTIONS\")\n",
    "print(\"=\" * 80)\n",
    "model_info = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Neural Network'],\n",
    "    'Type': ['Linear Classifier', 'Deep Learning'],\n",
    "    'Hyperparameters': ['max_iter=1000', 'hidden_layers=(100,50), max_iter=500'],\n",
    "    'Complexity': ['Low', 'High']\n",
    "})\n",
    "print(model_info.to_string(index=False))\n",
    "\n",
    "# Performance Metrics by Fold\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-VALIDATION METRICS (F1-SCORE)\")\n",
    "print(\"=\" * 80)\n",
    "fold_metrics = pd.DataFrame({\n",
    "    'Fold': range(1, k_folds + 1),\n",
    "    'LR F1-Score': np.round(lr_f1_scores, 4),\n",
    "    'NN F1-Score': np.round(nn_f1_scores, 4),\n",
    "    'Difference': np.round(np.array(nn_f1_scores) - np.array(lr_f1_scores), 4)\n",
    "})\n",
    "print(fold_metrics.to_string(index=False))\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': ['Mean F1-Score', 'Std Dev F1-Score', 'Mean Accuracy', 'Mean Precision', 'Mean Recall'],\n",
    "    'Logistic Regression': [\n",
    "        np.round(lr_f1_mean, 4),\n",
    "        np.round(lr_f1_std, 4),\n",
    "        np.round(np.mean(lr_accuracy), 4),\n",
    "        np.round(np.mean(lr_precision), 4),\n",
    "        np.round(np.mean(lr_recall), 4)\n",
    "    ],\n",
    "    'Neural Network': [\n",
    "        np.round(nn_f1_mean, 4),\n",
    "        np.round(nn_f1_std, 4),\n",
    "        np.round(np.mean(nn_accuracy), 4),\n",
    "        np.round(np.mean(nn_precision), 4),\n",
    "        np.round(np.mean(nn_recall), 4)\n",
    "    ]\n",
    "})\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# T-Test Results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INDEPENDENT SAMPLES T-TEST RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "ttest_results = pd.DataFrame({\n",
    "    'Test Parameter': ['t-statistic', 'p-value', 'Significance Level', 'Result', 'Conclusion'],\n",
    "    'Value': [\n",
    "        np.round(t_statistic, 4),\n",
    "        np.round(p_value, 4),\n",
    "        alpha,\n",
    "        'Significant' if is_significant else 'Not Significant',\n",
    "        'Reject H₀' if is_significant else 'Fail to reject H₀'\n",
    "    ]\n",
    "})\n",
    "print(ttest_results.to_string(index=False, header=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b8fa3",
   "metadata": {},
   "source": [
    "## Section 10: Discussion and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "1. **Model Performance Comparison:**\n",
    "   - Logistic Regression Mean F1-Score: 0.9718 (±0.0092)\n",
    "   - Neural Network Mean F1-Score: 0.9859 (±0.0021)\n",
    "   - The Neural Network achieved ~1.41% higher F1-score on average\n",
    "   - Neural Network shows more consistent performance (lower std dev)\n",
    "\n",
    "2. **Statistical Significance:**\n",
    "   - t-statistic = -3.3584, p-value = 0.01 < α(0.05)\n",
    "   - We REJECT the null hypothesis\n",
    "   - There IS a statistically significant difference between models\n",
    "   - The difference is unlikely due to random chance (only 1% probability)\n",
    "\n",
    "3. **Effect Size Interpretation:**\n",
    "   - Although statistically significant, the practical difference (1.41%) is small\n",
    "   - Both models achieved >97% F1-score, indicating excellent performance\n",
    "   - The additional complexity of Neural Network may not justify the marginal improvement\n",
    "\n",
    "4. **Model Selection Recommendation:**\n",
    "   - For production: Consider Logistic Regression for simplicity, speed, and interpretability\n",
    "   - For performance optimization: Neural Network provides statistically significant improvement\n",
    "   - Trade-off between complexity and performance gain should guide decision\n",
    "\n",
    "### Assumptions and Limitations:\n",
    "- T-test assumes normal distribution and equal variances (approximately met with k-fold CV)\n",
    "- Results are specific to 20 Newsgroups dataset; generalization to other domains needs validation\n",
    "- Small sample size (k=5 folds) may limit statistical power\n",
    "- Text preprocessing (TF-IDF, stopword removal) affects feature representation and model performance\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
